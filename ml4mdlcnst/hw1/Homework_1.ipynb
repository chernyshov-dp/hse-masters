{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RJn-WZZrt-nP"
   },
   "source": [
    "# Домашнее задание №1: линейная регрессия и векторное дифференцирование.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Pa-Lia6St-nR"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HLKQNyiVt-nR"
   },
   "source": [
    "## Многомерная линейная регрессия из sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qbVg93IAt-nR"
   },
   "source": [
    "Применим многомерную регрессию из sklearn для стандартного датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rCVSClXut-nS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 100) (10000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(n_samples = 10000)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m41sLfcJt-nS"
   },
   "source": [
    "У нас 10000 объектов и 100 признаков. Для начала решим задачу аналитически \"из коробки\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Xx3P6hOVt-nS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4153191393409055e-25\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "reg = LinearRegression().fit(X, y)\n",
    "print(mean_squared_error(y, reg.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNWLYeKst-nS"
   },
   "source": [
    "Теперь попробуем обучить линейную регрессию методом градиентного спуска \"из коробки\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "hK5BO3uxt-nS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.246479180400162e-12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-8.38654327e-08, -5.51883044e-09, -2.13333867e-08,  1.73398485e-08,\n",
       "        1.08029089e-08,  2.21901699e-08, -3.16431505e-08, -1.45648663e-08,\n",
       "       -1.09481239e-08, -1.84244413e-08, -2.59007135e-08,  7.68953347e+00,\n",
       "        3.08940798e-08, -2.03821823e-08,  3.51693609e-08,  1.74448130e-08,\n",
       "       -3.65375575e-08,  2.43080626e-08, -4.43754989e-09,  5.28439206e-08,\n",
       "       -5.95282914e-09, -3.84824518e-08, -2.03121756e-08,  3.76657526e-08,\n",
       "       -1.62217886e-08,  1.76704332e-08,  1.22977865e-08, -1.34962950e-08,\n",
       "        8.83785713e+01,  7.87209139e-09,  3.90931070e-08,  3.73041780e-08,\n",
       "        1.85952133e+01, -4.78183369e-08,  7.35388056e-09,  2.15118122e-08,\n",
       "       -1.42266033e-08,  4.77847181e-08,  7.27979395e+01,  2.25597834e-08,\n",
       "       -5.27757582e-09, -4.20839142e-08,  6.46997312e-09, -5.06536030e-08,\n",
       "       -4.54787493e-08, -2.68223563e-08,  2.89814406e-08,  5.92976645e-08,\n",
       "        1.59055122e-08,  7.02494999e+01,  2.93440801e-08,  3.16567019e-08,\n",
       "        6.96770111e-08, -8.25804101e-08, -1.78041157e-08, -2.67875796e-08,\n",
       "        4.46659167e-08,  9.36510412e+01,  4.53759452e+01, -6.31166098e-09,\n",
       "       -7.87824850e-09,  8.39569545e+01, -7.84678384e-08, -7.71972623e-09,\n",
       "        1.35623711e-08, -2.42103543e-08,  6.47585771e-08, -2.58931233e-10,\n",
       "       -8.74346777e-09,  5.85202874e+00,  7.60361684e-08,  5.03214967e-09,\n",
       "       -2.29451499e-08,  3.39775366e-09,  1.06403498e-08, -2.15053698e-08,\n",
       "       -2.26013637e-08,  3.33305111e-08, -2.62457915e-10,  1.83773098e-08,\n",
       "        2.57648330e-08,  1.08941317e-08,  3.50602242e-08, -3.11297754e-08,\n",
       "        5.06815540e-09, -5.22658320e-08,  1.45668499e-08,  5.07510574e-10,\n",
       "       -2.44136319e-08,  7.01020364e-08, -2.17433481e-08,  7.97307277e+01,\n",
       "        5.12552192e-09,  1.91108407e-08,  7.02459170e-08,  2.29638800e-09,\n",
       "       -1.47062184e-08, -5.45082502e-08, -3.77383381e-08, -4.33491326e-08])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "reg = SGDRegressor(alpha=0.00000001).fit(X, y)\n",
    "print(mean_squared_error(y, reg.predict(X)))\n",
    "reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-MCjcHrSt-nS"
   },
   "source": [
    "***Задание 1 (0.5 балла).*** Объясните, чем вызвано различие двух полученных значений метрики?\n",
    "\n",
    "***Задание 2 (0.5 балла).*** Подберите гиперпараметры в методе градиентного спуска так, чтобы значение MSE было близко к значению MSE, полученному при обучении LinearRegression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ответ 1:***\n",
    "\n",
    "Для ```SGDRegressor``` выбрано слишком большое значение параметра регуляризации ```alpha```, которое может привести к медленной сходимости модели или даже застреванию в локальном минимуме. Кроме того, в методе градиентного спуска (```SGDRegressor```) веса инициализируются случайным образом, это может привести к тому, что модель начинает обучение не с оптимальных весов. В аналитическом решении (```LinearRegression```) веса инициализируются оптимально сразу, используя формулу нормального уравнения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ответ 2:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "α = 0.01; MSE (SGDRegressor) = 4.529973255985326\n",
      "\n",
      "α = 0.001; MSE (SGDRegressor) = 0.044859996691931886\n",
      "\n",
      "α = 0.0001; MSE (SGDRegressor) = 0.00043487612105318316\n",
      "\n",
      "α = 1e-05; MSE (SGDRegressor) = 4.492913102550342e-06\n",
      "\n",
      "α = 1.0000000000000002e-06; MSE (SGDRegressor) = 4.339679313498995e-08\n",
      "\n",
      "α = 1.0000000000000002e-07; MSE (SGDRegressor) = 4.1074745160134284e-10\n",
      "\n",
      "α = 1.0000000000000002e-08; MSE (SGDRegressor) = 4.24515358240008e-12\n",
      "\n",
      "α = 1.0000000000000003e-09; MSE (SGDRegressor) = 4.2153691302830133e-14\n",
      "\n",
      "α = 1.0000000000000003e-10; MSE (SGDRegressor) = 4.418123569810853e-16\n",
      "\n",
      "α = 1.0000000000000003e-11; MSE (SGDRegressor) = 4.513738059356373e-18\n",
      "\n",
      "α = 1.0000000000000002e-12; MSE (SGDRegressor) = 5.172318323009637e-20\n",
      "\n",
      "α = 1.0000000000000002e-13; MSE (SGDRegressor) = 1.3823395763244954e-21\n",
      "\n",
      "α = 1.0000000000000002e-14; MSE (SGDRegressor) = 7.5244534761252305e-25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Подбираем параметр alpha для SGDRegressor\n",
    "reg = LinearRegression().fit(X, y)\n",
    "mse_reg = mean_squared_error(y, reg.predict(X))\n",
    "\n",
    "alpha = 0.1\n",
    "sgd_reg = SGDRegressor(alpha=alpha).fit(X, y)\n",
    "mse_sgd_reg = mean_squared_error(y, sgd_reg.predict(X))\n",
    "\n",
    "while (mse_sgd_reg - mse_reg) > 1e-24:\n",
    "    alpha /= 10\n",
    "    sgd_reg = SGDRegressor(alpha=alpha).fit(X, y)\n",
    "    mse_sgd_reg = mean_squared_error(y, sgd_reg.predict(X))\n",
    "    print(f'α = {alpha}; MSE (SGDRegressor) = {mse_sgd_reg}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С уменьшением ```alpha```, уменьшается ```MSE```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.377705537895378e-25\n"
     ]
    }
   ],
   "source": [
    "# Обучаем при alpha=1e-14\n",
    "reg = SGDRegressor(alpha=1e-14).fit(X, y)\n",
    "print(mean_squared_error(y, reg.predict(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.610094468791942e-25\n"
     ]
    }
   ],
   "source": [
    "# Обучаем при alpha=0\n",
    "reg = SGDRegressor(alpha=0).fit(X, y)\n",
    "print(mean_squared_error(y, reg.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xG7V0qzut-nT"
   },
   "source": [
    "## Ваша многомерная линейная регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJqNQMGHt-nT"
   },
   "source": [
    "***Задание 3 (5 баллов)***. Напишите собственную многомерную линейную регрессию, оптимизирующую MSE методом *градиентного спуска*. Для этого используйте шаблонный класс.\n",
    "\n",
    "Критерий останова: либо норма разности весов на текущей и предыдущей итерациях меньше определенного значения (первый и третий варианты), либо модуль разности функционалов качества (MSE) на текущей и предыдущей итерациях меньше определенного значения (второй и четвертый варианты). Также предлагается завершать обучение в любом случае, если было произведено слишком много итераций.\n",
    "\n",
    "***Задание 4 (2 балла)***. Добавьте l1 регуляризацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LinearRegression:\n",
    "    def __init__(self, alpha=0.0001, l_ratio=0.001, tol=0.001, max_iter=1000):\n",
    "        \"\"\"\n",
    "        Для начала необходимо инициализировать параметры\n",
    "        alpha - это learning rate или шаг обучения\n",
    "        l_ratio - коэффициент L1 регуляризации\n",
    "        tol - значение для критерия останова\n",
    "        max_iter - максимальное количество итераций обучения\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.l_ratio = l_ratio\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Метод для обучения линейной регрессии\n",
    "        X - матрица признаков\n",
    "        y - вектор правильных ответов\n",
    "        \"\"\"\n",
    "        # Добавляем столбец единиц для учета свободного члена\n",
    "        X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "        \n",
    "        # Инициализация случайных весов\n",
    "        self.weights = np.random.randn(X.shape[1])\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "            y_pred = np.dot(X, self.weights)\n",
    "            gradient = np.dot(X.T, (y_pred - y)) / X.shape[0]\n",
    "\n",
    "            # L1-регуляризация для весов, исключая свободный член\n",
    "            gradient[1:] += self.l_ratio * np.sign(self.weights[1:])\n",
    "\n",
    "            prev_weights = self.weights.copy()\n",
    "            self.weights -= self.alpha * gradient\n",
    "\n",
    "            if np.linalg.norm(self.weights - prev_weights) < self.tol:\n",
    "                break\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Метод для предсказаний линейной регрессии\n",
    "        X - матрица признаков\n",
    "        \"\"\"\n",
    "        # Добавляем столбец единиц для учета свободного члена\n",
    "        X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "        return np.dot(X, self.weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "_45PJNw1t-nT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 1.4001446771940016e-05\n",
      "You are amazing! Great work!\n"
     ]
    }
   ],
   "source": [
    "my_reg = LinearRegression(alpha=0.1, l_ratio=0.001, tol=0.001, max_iter=1000)\n",
    "my_reg.fit(X, y)\n",
    "mse_my_reg = mean_squared_error(y, my_reg.predict(X))\n",
    "print(f\"MSE: {mse_my_reg}\")\n",
    "assert mse_my_reg < 1e-3\n",
    "print('You are amazing! Great work!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-LvqfXkt-nT"
   },
   "source": [
    "***Задание 5 (1 балл)***. Обучите линейную регрессию из коробки\n",
    "\n",
    "* с l1-регуляризацией (from sklearn.linear_model import Lasso)\n",
    "* со значением параметра регуляризации 0.1\n",
    "\n",
    "Обучите вашу линейную регрессию с тем же значением параметра регуляризации и сравните результаты. Сделайте выводы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "vXFGXaAwt-nT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (Lasso) = 0.09926767786325631;\n",
      "MSE (MyReg) = 0.10252518632022598\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lasso_reg = Lasso(alpha=0.1).fit(X, y)\n",
    "mse_lasso_reg = mean_squared_error(y, lasso_reg.predict(X))\n",
    "\n",
    "my_reg = LinearRegression(alpha=0.1, l_ratio=0.1, tol=0.001, max_iter=1000)\n",
    "my_reg.fit(X, y)\n",
    "mse_my_reg = mean_squared_error(y, my_reg.predict(X))\n",
    "\n",
    "print(f'MSE (Lasso) = {mse_lasso_reg};\\nMSE (MyReg) = {mse_my_reg}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Библиотечная регрессия показала лучший ```MSE```. Возможных причин масса: более оптимальное значение ```learning_rate```, различия в способе инициализации весов, больше итераций обучения и т. д."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
